{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b87f82d2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import pallas as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "846c2c40-ea0d-4250-b1be-1e89d5820464",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create small tensors for testing\n",
    "BATCH = 1\n",
    "HEADS = 1\n",
    "SEQ_LEN = 8\n",
    "HEAD_DIM = 4\n",
    "\n",
    "print(\"Creating random tensors\")\n",
    "K = jax.random.normal(jax.random.key(0), (BATCH, HEADS, SEQ_LEN, HEAD_DIM))\n",
    "V = jax.random.normal(jax.random.key(1), (BATCH, HEADS, SEQ_LEN, HEAD_DIM))\n",
    "Q = jax.random.normal(jax.random.key(2), (BATCH, HEADS, SEQ_LEN, HEAD_DIM))\n",
    "print(\"Random tensors created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1346db5-ca47-418a-a650-5e3675bb75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "K[0][0][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a8af869-565f-41c3-b147-a987c2442ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q[0][0][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c74e2bf4-1c19-4674-9fab-38587ce846b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "V[0][0][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7304b51c",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=IvgV6QcsC64\n",
    "\n",
    "![./einsum_overview.png](einsum_overview.png)\n",
    "![./einsum_element_wise_multiplication.png](einsum_element_wise_multiplication.png)\n",
    "![./einsum_rule_1.png](einsum_rule_1.png)\n",
    "![./einsum_rule_3-4.png](einsum_rule_3-4.png)\n",
    "![./einsum_summation_along_axis.png](einsum_summation_along_axis.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2fd1b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix a\n",
    "# create a 3x3 matrix\n",
    "a = [[1,2,3],\n",
    "     [4,5,6],\n",
    "     [7,8,9]]\n",
    "\n",
    "# Matrix b\n",
    "b = [[1,2,3],\n",
    "     [4,5,6],\n",
    "     [7,8,9]]\n",
    "\n",
    "print(\"Method 1: Using nested for loops\")\n",
    "# Initialize result matrix with zeros\n",
    "result_loops = [[0 for x in range(len(b[0]))] for y in range(len(a))]\n",
    "\n",
    "# Iterate through rows of a\n",
    "for i in range(len(a)):\n",
    "    # Iterate through columns of b\n",
    "    for j in range(len(b[0])):\n",
    "        # Iterate through rows of b\n",
    "        for k in range(len(b)):\n",
    "            result_loops[i][j] += a[i][k] * b[k][j]\n",
    "\n",
    "print(\"Result using for loops:\")\n",
    "for row in result_loops:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nMethod 2: Using numpy einsum\")\n",
    "import numpy as np\n",
    "a_np = np.array(a)\n",
    "b_np = np.array(b)\n",
    "result_einsum = np.einsum('ik,kj->ij', a_np, b_np)\n",
    "\n",
    "print(\"Result using einsum:\")\n",
    "print(result_einsum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcb565af-e595-4ae1-b880-ddca780ccf14",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def attention_ourselves(_Q, _K, _V):\n",
    "    print(\"Computing attention weights\")\n",
    "    print(\"Q shape:\", _Q.shape)\n",
    "    print(\"K shape:\", _K.shape)\n",
    "    print(\"V shape:\", _V.shape)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Dimensions:\n",
    "    # _Q, _K, _V are (batch, seq_len, heads, head_dim)\n",
    "    # _weights_unnormalized is (batch, heads, seq_len, seq_len)\n",
    "    # _weights after softmax is (batch, heads, seq_len, seq_len)\n",
    "    # output is (batch, seq_len, heads, head_dim)\n",
    "\n",
    "    # Step 1: Q * K^T to get attention weights\n",
    "    # Memory: 2 * (2*batch*seq*head_dim) for Q,K\n",
    "    # Flops: 2*batch*seq*seq*head_dim\n",
    "    _weights_unnormalized = jax.numpy.einsum(\"bshd,bthd->bhst\", _Q, _K)\n",
    "    _weights = jax.nn.softmax(_weights_unnormalized)\n",
    "\n",
    "    # Step 2: weights * V to get final output\n",
    "    # Memory: 2*batch*heads*seq^2 for loading weights (seq^2 because weights matrix is seq_len x seq_len)\n",
    "    # Flops: 2*batch*seq*seq*head_dim\n",
    "    output = jax.numpy.einsum(\"bhst,bshd->bshd\", _weights, _V)\n",
    "\n",
    "    print(\"Weights computed successfully\")\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"\\nDimension Analysis:\")\n",
    "    print(\"Input shapes (Q,K,V): batch x seq_len x heads x head_dim\")\n",
    "    print(\"Weight matrix shape: batch x heads x seq_len x seq_len\")\n",
    "    print(\"Output shape: batch x seq_len x heads x head_dim\")\n",
    "\n",
    "    # Calculate and print memory bandwidth, flops, and arithmetic intensity\n",
    "    batch, seq_len, heads, head_dim = _Q.shape\n",
    "\n",
    "    print(\"\\nDetailed Analysis:\")\n",
    "    print(\"Step 1: Q * K^T (Attention Weights Calculation)\")\n",
    "    mem_bandwidth_step1 = 2 * (2 * batch * seq_len * heads * head_dim) + (2 * batch * heads * seq_len * seq_len)\n",
    "    flops_step1 = 2 * batch * heads * seq_len * seq_len * head_dim\n",
    "    ai_step1 = flops_step1 / mem_bandwidth_step1\n",
    "    print(f\"  Memory Bandwidth: {mem_bandwidth_step1} units\")\n",
    "    print(f\"  FLOPS: {flops_step1} operations\")\n",
    "    print(f\"  Arithmetic Intensity: {ai_step1:.2f}\")\n",
    "\n",
    "    print(\"\\nStep 2: weights * V (Final Output Calculation)\")\n",
    "    mem_bandwidth_step2 = (2 * batch * heads * seq_len * seq_len) + (2 * batch * seq_len * heads * head_dim)\n",
    "    flops_step2 = 2 * batch * heads * seq_len * seq_len * head_dim\n",
    "    ai_step2 = flops_step2 / mem_bandwidth_step2\n",
    "    print(f\"  Memory Bandwidth: {mem_bandwidth_step2} units\")\n",
    "    print(f\"  FLOPS: {flops_step2} operations\")\n",
    "    print(f\"  Arithmetic Intensity: {ai_step2:.2f}\")\n",
    "\n",
    "    print(f\"\\nAssuming seq_len ({seq_len}) >> head_dim ({head_dim}):\")\n",
    "    print(f\"  Approximate Arithmetic Intensity: ~{head_dim}\")\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50ff0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calling attention function\")\n",
    "result = attention_ourselves(Q, K, V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a6ea3",
   "metadata": {},
   "source": [
    "# Analysis of Coding\n",
    "\n",
    "## Inputs are:\n",
    "* 3x Batch, Sequence, HeadDim (Q,K,V)\n",
    "\n",
    "## Outputs are:\n",
    "* Batch, Sequence, HeadDim.\n",
    "\n",
    "## Intermediate output is:\n",
    "* Output dimension Batch, Sequence, Sequence = W = softmax(einsum(Q,V))\n",
    "* Memory bandwidth = 2 * (2* Batch * Sequence * HeadDim) + (2* Batch * Sequence^2)\n",
    "* Flops are 2* Batch * Sequence * Sequence * HeadDim\n",
    "* Assuming Seq >> HeadDim, Arithmetic intensity is ~HeadDim.\n",
    "\n",
    "## Then W*V:\n",
    "* Output is Batch, Sequence, HeadDim = einsum(W,V)\n",
    "* Flops are 2* Batch * Sequence * Sequence * HeadDim\n",
    "* Memory bandwidth again dominated by (2* Batch * Sequence^2) (loading W)\n",
    "* So Arithmetic Intensity Again ~HeadDim\n",
    "\n",
    "\n",
    "## Overall bandwidth:\n",
    "* Inputs are 3*Batch*Sequence*HeadDim\n",
    "* Outputs are 1*Batch*Sequence*HeadDim\n",
    "* So 8*Batch*Sequence*HeadDim bytes.\n",
    "\n",
    "## Overall flops:\n",
    "* 4*Batch*Sequence*Sequence*HeadDim\n",
    "\n",
    "## Overall ratio flops/byte:\n",
    "* Sequence / 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d3365",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Many Many Solutions Emerged!\n",
    "\n",
    "* The key problem is that tensors of size Batch, Sequence, Sequence are too big to write to HBM efficiently.\n",
    "* What we need is a fused kernel that allows us to not write back to HBM\n",
    "* The simplest fused schedule is actually to notice that with Sequence=2048, we can handle each example independently and the tensors are only as large as 2048*2028*2 = 8.3 MB.\n",
    "    * We have 160MB of SRAM - no reason we should be writing anything back to HBM.\n",
    "    * This trick doesn't actually work that well - at Sequence =16384 we'd need 536 MB so we need to use HBM.\n",
    "* The most famous and widely used is fused schedule is FlashAttention (Tri Dao et al, 2022)\n",
    "    * This depends on some clever observations about softmax and actually fully breaks the memory dependence on Batch*Sequence^2.\n",
    "    * Nowadays there are many variants of FlashAttention that all exploit the same observation about softmax.\n",
    "    * (We can cover FlashAttention in detail at some point if folks want. I'll hold a poll once I cover all the basic topics!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da9ccd",
   "metadata": {},
   "source": [
    "# Review of Attention Perf\n",
    "\n",
    "* The key problem is that tensors of size Batch, Sequence, Sequence are too big to write to HBM efficiently.\n",
    "* What we need is a fused kernel that allows us to not write back to HBM\n",
    "* The simplest fused schedule is actually to notice that with Sequence=2048, we can handle each example independently and the tensors are only as large as 2048*2028*2 = 8.3 MB.\n",
    "    * We have 160MB of SRAM - no reason we should be writing anything back to HBM.\n",
    "    * This trick doesn't actually work that well - at Sequence =16384 we'd need 536 MB so we need to use HBM.\n",
    "* The most famous and widely used is fused schedule is FlashAttention (Tri Dao et al, 2022)\n",
    "    * This depends on some clever observations about softmax and actually fully breaks the memory dependence on Batch*Sequence^2.\n",
    "    * Nowadays there are many variants of FlashAttention that all exploit the same observation about softmax.\n",
    "    * (We can cover FlashAttention in detail at some point if folks want. I'll hold a poll once I cover all the basic topics!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd17eb5",
   "metadata": {},
   "source": [
    "# Review of Attention Usefulness\n",
    "\n",
    "* This version of attention is totally order invariant!\n",
    "    * To be useful we will need to add positional encodings - so each tensor is representing where it comes from.\n",
    "        * (Attention will still be order invariant)\n",
    "        * This is a bizarre but useful trait of attention!\n",
    "* This version of attention is not causal!\n",
    "    * Easy to add - zero out unwanted W_unnormalized's\n",
    "* This version of attention doesn't support \"multiprompt packing\" - training on multiple sequences in one example.\n",
    "    * Also easy to add - zero out unwanted W_unnormalized's\n",
    "* Endless more tricks in Attention! But these (and Flash variants) are the top 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51d51310-a95c-4d7f-a4ff-32ddc8dc4476",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def attention_ourselves_causal(_Q, _K, _V):\n",
    "    # Dimensions:\n",
    "    # _Q, _K, _V are (batch, seq_len, heads, head_dim)\n",
    "    # _weights_unnormalized is (batch, heads, seq_len, seq_len)\n",
    "    # _weights after softmax is (batch, heads, seq_len, seq_len)\n",
    "    # output is (batch, seq_len, heads, head_dim)\n",
    "\n",
    "    # Step 1: Q * K^T to get attention weights\n",
    "    _weights_unnormalized = jax.numpy.einsum(\"bshd,bthd->bhst\", _Q, _K)\n",
    "    _weights_unnormalized_to_zero_out = jax.numpy.triu(jax.numpy.ones((SEQ_LEN, SEQ_LEN), jax.numpy.bfloat16), 1)\n",
    "    _weights = jax.nn.softmax(_weights_unnormalized - 1e6 * _weights_unnormalized_to_zero_out)\n",
    "\n",
    "    # Step 2: weights * V to get final output\n",
    "    output = jax.numpy.einsum(\"bhst,bshd->bshd\", _weights, _V)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "deea6c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_ourselves_causal = attention_ourselves_causal(Q, K, V)\n",
    "# attn_value = pl.attention.mha_reference(Q, K, V, segment_ids=None, causal=True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
